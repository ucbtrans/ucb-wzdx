{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Events Json to CSV\n",
    "- Input: the `.json` file generated by the 511 API: https://api.511.org/traffic/wzdx?api_key=789a13cf-d2e0-46db-95f2-54c2611933c1\n",
    "    - The input file should be stored under the folder \"wzdx_511\"\n",
    "- Output: A `.csv` file with all the information, which can be import into MySQL Database\n",
    "    - The output location is `road_events/road_events_511.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'wzdx_511'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 15 minutes' running time\n",
    "file_names = []\n",
    "csv_file_path = 'road_events/road_events_511_5.csv'\n",
    "\n",
    "    # Define the fieldnames for the CSV\n",
    "fieldnames = [\n",
    "    'id', 'type', 'data_source_id', 'event_type', 'road_names',\n",
    "    'direction', 'description', 'creation_date', 'update_date',\n",
    "    'start_date', 'end_date', 'event_status', 'start_date_accuracy',\n",
    "    'end_date_accuracy', 'beginning_accuracy', 'ending_accuracy',\n",
    "    'location_method', 'vehicle_impact', 'beginning_cross_street',\n",
    "    'ending_cross_street','type_name',  'are_workers_present', 'worker_presence_definition',\n",
    "    'worker_presence_confidence' , 'worker_presence_last_confirmed_date', 'geometry_type', 'geometry_coordinates'\n",
    "]\n",
    "\n",
    "def str_to_tinyint(s):\n",
    "    if s is True or s == \"True\":\n",
    "        return 1\n",
    "    elif s is False or s == \"False\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 'NULL'\n",
    "\n",
    "def parse_datetime(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str,'%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except (TypeError, ValueError):\n",
    "        return 'NULL'\n",
    "    \n",
    "# Function to check if a row already exists in the CSV\n",
    "def row_exists(csv_file_path, row_id):\n",
    "    try:\n",
    "        with open(csv_file_path, mode='r', newline='') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for existing_row in reader:\n",
    "                if existing_row['id'] == row_id:\n",
    "                    return True\n",
    "    except FileNotFoundError:\n",
    "        # File not found, meaning no rows exist yet\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r') as file:\n",
    "        geojson_data = json.load(file)\n",
    "        # Define the CSV file path\n",
    "\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_file_path, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Iterate through each feature in the GeoJSON file\n",
    "        for feature in geojson_data['features']:\n",
    "            flag = 1\n",
    "            # Extract core details and flatten the data\n",
    "            core_details = feature['properties']['core_details']\n",
    "            properties = feature['properties']\n",
    "            geometry = feature['geometry']\n",
    "            row_id = feature['id']\n",
    "            if 'worker_presence' not in feature['properties']:\n",
    "                feature['properties']['worker_presence'] = 1\n",
    "                # If you have a specific flag variable, set it here\n",
    "                flag = 0 # Get through all the rows with worker_presence\n",
    "            else:\n",
    "                worker_presence_wzdx = feature['properties']['worker_presence']\n",
    "            # Check if the row already exists in the CSV\n",
    "            if row_exists(csv_file_path, row_id):\n",
    "                # Skip the row if it already exists\n",
    "                continue\n",
    "            \n",
    "            # Prepare the row to be written to the CSV\n",
    "            if flag == 1:\n",
    "                row = {\n",
    "                    'id': feature['id'],\n",
    "                    'type': feature['type'],\n",
    "                    'data_source_id': core_details.get('data_source_id'),\n",
    "                    'event_type': core_details.get('event_type'),\n",
    "                    'road_names': ', '.join(core_details.get('road_names', [])),\n",
    "                    'direction': core_details.get('direction'),\n",
    "                    'description': core_details.get('description'),\n",
    "                    'creation_date': parse_datetime(core_details.get('creation_date')),\n",
    "                    'update_date': parse_datetime(core_details.get('update_date')),\n",
    "                    'start_date': parse_datetime(properties.get('start_date')),\n",
    "                    'end_date': parse_datetime(properties.get('end_date')),\n",
    "                    'event_status': properties.get('event_status'),\n",
    "                    'start_date_accuracy': properties.get('start_date_accuracy'),\n",
    "                    'end_date_accuracy': properties.get('end_date_accuracy'),\n",
    "                    'beginning_accuracy': properties.get('beginning_accuracy'),\n",
    "                    'ending_accuracy': properties.get('ending_accuracy'),\n",
    "                    'location_method': properties.get('location_method'),\n",
    "                    'vehicle_impact': properties.get('vehicle_impact'),\n",
    "                    'beginning_cross_street': properties.get('beginning_cross_street'),\n",
    "                    'ending_cross_street': properties.get('ending_cross_street'),\n",
    "                    'type_name': ', '.join([tw['type_name'] for tw in properties.get('types_of_work', [])]),\n",
    "                    'are_workers_present': worker_presence_wzdx.get('are_workers_present'),\n",
    "                    'worker_presence_definition': ', '.join(worker_presence_wzdx.get('definition', [])),\n",
    "                    'worker_presence_confidence': worker_presence_wzdx.get('confidence'),\n",
    "                    'worker_presence_last_confirmed_date': parse_datetime(worker_presence_wzdx.get('worker_presence_last_confirmed_date')),\n",
    "                    'geometry_type': geometry['type'],\n",
    "                    'geometry_coordinates': ', '.join([f\"[{', '.join(map(str, coord))}]\" for coord in geometry['coordinates']])\n",
    "                }\n",
    "            else:\n",
    "                row = {\n",
    "                    'id': feature['id'],\n",
    "                    'type': feature['type'],\n",
    "                    'data_source_id': core_details.get('data_source_id'),\n",
    "                    'event_type': core_details.get('event_type'),\n",
    "                    'road_names': ', '.join(core_details.get('road_names', [])),\n",
    "                    'direction': core_details.get('direction'),\n",
    "                    'description': core_details.get('description'),\n",
    "                    'creation_date': parse_datetime(core_details.get('creation_date')),\n",
    "                    'update_date': parse_datetime(core_details.get('update_date')),\n",
    "                    'start_date': parse_datetime(properties.get('start_date')),\n",
    "                    'end_date': parse_datetime(properties.get('end_date')),\n",
    "                    'event_status': properties.get('event_status'),\n",
    "                    'start_date_accuracy': properties.get('start_date_accuracy'),\n",
    "                    'end_date_accuracy': properties.get('end_date_accuracy'),\n",
    "                    'beginning_accuracy': properties.get('beginning_accuracy'),\n",
    "                    'ending_accuracy': properties.get('ending_accuracy'),\n",
    "                    'location_method': properties.get('location_method'),\n",
    "                    'vehicle_impact': properties.get('vehicle_impact'),\n",
    "                    'beginning_cross_street': properties.get('beginning_cross_street'),\n",
    "                    'ending_cross_street': properties.get('ending_cross_street'),\n",
    "                    'type_name': ', '.join([tw['type_name'] for tw in properties.get('types_of_work', [])]),\n",
    "                    'are_workers_present': 'NULL',\n",
    "                    'worker_presence_definition': 'NULL',\n",
    "                    'worker_presence_confidence': 'NULL',\n",
    "                    'worker_presence_last_confirmed_date': 'NULL'\n",
    "                }\n",
    "           \n",
    "        \n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the path for the re-uploaded road_events CSV\n",
    "re_uploaded_road_events_csv_path = 'road_events/road_events_511_5.csv'\n",
    "\n",
    "# Function to remove rows where the first columns match specific values\n",
    "def remove_rows_with_specific_columns(csv_path, columns_to_match):\n",
    "    temp_path = csv_path + \".tmp\"\n",
    "    with open(csv_path, 'r', newline='') as input_file, open(temp_path, 'w', newline='') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        first_row = True\n",
    "        for row in csv.reader(input_file):\n",
    "            # Check if the row's first columns match the specified values\n",
    "            if first_row:\n",
    "                writer.writerow(row)\n",
    "                first_row = False\n",
    "                continue\n",
    "            if row[:len(columns_to_match)] != columns_to_match:\n",
    "                writer.writerow(row)\n",
    "    # Replace the original file with the modified temp file\n",
    "    os.replace(temp_path, csv_path)\n",
    "\n",
    "# Columns to match for deletion\n",
    "columns_to_match = ['id', 'type', 'data_source_id']\n",
    "\n",
    "# Call the function to remove the specified rows from the re-uploaded road_events.csv\n",
    "remove_rows_with_specific_columns(re_uploaded_road_events_csv_path, columns_to_match)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
